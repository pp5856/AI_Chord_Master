import os
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from data_generator import ChordDataGenerator

# ==========================================
# ì„¤ì • (Configuration)
# ==========================================
DATA_DIR = "c:/AI_PROJECT/data/cqt_numpy"
MODEL_SAVE_PATH = "c:/AI_PROJECT/models/chord_model.h5"
BATCH_SIZE = 32
EPOCHS = 20           # ë¯¸ì„¸ ì¡°ì •ì€ ì„¬ì„¸í•˜ê²Œ ì˜¤ë˜ í•´ì•¼ í•˜ë¯€ë¡œ íšŸìˆ˜ë¥¼ ëŠ˜ë¦½ë‹ˆë‹¤.
LEARNING_RATE = 0.0001 # ë‡Œë¥¼ ë…¹ì˜€ìœ¼ë‹ˆ, ì§€ì‹ì´ ë§ê°€ì§€ì§€ ì•Šê²Œ ì•„ì£¼ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ(ë‚®ì€ í•™ìŠµë¥ ) ê³µë¶€í•©ë‹ˆë‹¤.
INPUT_SHAPE = (84, 84, 1) # ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ì„¸ë¡œ, ê°€ë¡œ, í‘ë°±1ì±„ë„)
NUM_CLASSES = 24      # ë¶„ë¥˜í•  ì½”ë“œ ê°œìˆ˜ (Major 12ê°œ + Minor 12ê°œ ë“± ë°ì´í„°ì— ë”°ë¼ ìë™ ê²°ì •ë¨)

def build_model(num_classes):
    """
    AI ëª¨ë¸(ë‡Œ)ì„ ì¡°ë¦½í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ResNet50ì´ë¼ëŠ” ì²œì¬ì˜ ë‡Œë¥¼ ë¹Œë ¤ì™€ì„œ(ì „ì´ í•™ìŠµ), ìš°ë¦¬ ëª©ì ì— ë§ê²Œ ê°œì¡°í•©ë‹ˆë‹¤.
    """
    # 1. ì…ë ¥ì¸µ ì •ì˜ 
    # í‘ë°± ì´ë¯¸ì§€(1ì±„ë„)ë¥¼ ë°›ì§€ë§Œ, ResNetì€ ì»¬ëŸ¬(3ì±„ë„)ë¥¼ ì¢‹ì•„í•˜ë¯€ë¡œ 3ì¥ ê²¹ì³ì„œ í‰ë‚´ëƒ…ë‹ˆë‹¤.
    input_tensor = Input(shape=INPUT_SHAPE)
    x = tf.keras.layers.Conv2D(3, (3, 3), padding='same')(input_tensor) # 1ì±„ë„ -> 3ì±„ë„ë¡œ ë»¥íŠ€ê¸°
    
    # 2. ë² ì´ìŠ¤ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° 
    # include_top=False: "ë§ˆì§€ë§‰ ë¶„ë¥˜ê¸°(ê°œ/ê³ ì–‘ì´ ë§ì¶”ëŠ” ë¶€ë¶„)ëŠ” ë–¼ê³  ê°€ì ¸ì™€"
    # weights='imagenet': "ì´ë¯¸ì§€ë„· ë°ì´í„°ë¡œ ë¯¸ë¦¬ ê³µë¶€í•œ ì§€ì‹ì„ ê°€ì ¸ì™€"
    # ì£¼ì˜: input_tensorë¥¼ ì§ì ‘ ë„£ìœ¼ë©´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆì–´ì„œ, input_shapeë§Œ ì§€ì •í•˜ê³  ë”°ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(84, 84, 3))
    x = base_model(x)
    
    # 3. ë² ì´ìŠ¤ ëª¨ë¸ ë…¹ì´ê¸° (Fine-Tuning)
    # "ì²œì¬ì˜ ì§€ì‹ì„ ìš°ë¦¬ ë¬¸ì œì— ë§ê²Œ ì¡°ê¸ˆë§Œ ìˆ˜ì •í•˜ì"
    # ì²˜ìŒë¶€í„° ë‹¤ í•™ìŠµí•˜ë©´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ, ì´ë¯¸ 1ì°¨ í•™ìŠµì„ í–ˆê±°ë‚˜ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ë©´
    # ì „ì²´ë¥¼ ë¯¸ì„¸í•˜ê²Œ ì¡°ì •(Fine-Tuning)í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ í›¨ì”¬ ì¢‹ìŠµë‹ˆë‹¤.
    base_model.trainable = True 
    
    # 4. ìš°ë¦¬ë§Œì˜ ë¶„ë¥˜ê¸° ë¶™ì´ê¸° (ë¨¸ë¦¬)
    # x = base_model.output # ìœ„ì—ì„œ ì´ë¯¸ ì—°ê²°í–ˆìœ¼ë¯€ë¡œ ì´ ì¤„ì€ í•„ìš” ì—†ìŒ
    x = GlobalAveragePooling2D()(x) # ì´ë¯¸ì§€ íŠ¹ì§•ì„ ì••ì¶• (í‰ê· ë‚´ê¸°)
    x = Dense(1024, activation='relu')(x) # ìƒê°í•  ë‰´ëŸ° 1024ê°œ ì¶”ê°€
    x = Dropout(0.5)(x) # ê³¼ì™¸ ê³µë¶€ ë„ˆë¬´ ë§ì´ í•´ì„œ ë©ì²­í•´ì§€ëŠ” ê²ƒ(ê³¼ì í•©) ë°©ì§€
    
    # 5. ìµœì¢… ì¶œë ¥ì¸µ (ì…)
    # num_classesê°œì˜ í™•ë¥ ì„ ë±‰ì–´ëƒ„ (ì˜ˆ: Cì½”ë“œì¼ í™•ë¥  80%, Amì¼ í™•ë¥  5%...)
    predictions = Dense(num_classes, activation='softmax')(x)
    
    # ëª¨ë¸ ì¡°ë¦½ ì™„ë£Œ
    model = Model(inputs=input_tensor, outputs=predictions)
    return model

def main():
    # 1. ëª¨ë¸ ì €ì¥í•  í´ë” ë§Œë“¤ê¸°
    os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
    
    # 2. ë°ì´í„° ì œë„ˆë ˆì´í„° ì¤€ë¹„ (ê¸‰ì‹ ë‹¹ë²ˆ)
    # ì „ì²´ ë°ì´í„°ì˜ 20%ëŠ” ê²€ì¦ìš©(Validation)ìœ¼ë¡œ ë”°ë¡œ ë¹¼ë‘¡ë‹ˆë‹¤.
    # í•™ìŠµìš© (80%)
    train_generator = ChordDataGenerator(
        data_dir=DATA_DIR,
        batch_size=BATCH_SIZE,
        input_shape=INPUT_SHAPE,
        shuffle=True,
        validation_split=0.2,
        subset='training'
    )
    
    # ê²€ì¦ìš© (20%)
    validation_generator = ChordDataGenerator(
        data_dir=DATA_DIR,
        batch_size=BATCH_SIZE,
        input_shape=INPUT_SHAPE,
        shuffle=False, # ê²€ì¦í•  ë•ŒëŠ” êµ³ì´ ì„ì„ í•„ìš” ì—†ìŒ
        validation_split=0.2,
        subset='validation'
    )
    
    # í´ë˜ìŠ¤ ê°œìˆ˜ ìë™ íŒŒì•…
    real_num_classes = train_generator.num_classes
    print(f"ë¶„ë¥˜í•  í´ë˜ìŠ¤ ê°œìˆ˜: {real_num_classes}ê°œ")
    
    # 3. ëª¨ë¸ ë§Œë“¤ê¸°
    model = build_model(real_num_classes)
    
    # 4. í•™ìŠµ ë°©ë²• ì„¤ì • (Compile)
    # optimizer='adam': ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ ê³µë¶€ë²•
    # loss='categorical_crossentropy': ê°ê´€ì‹ ë¬¸ì œ í‹€ë¦° ì •ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë²•
    # metrics=['accuracy']: "ëª‡ ì  ë§ì•˜ë‹ˆ?" (ì •í™•ë„)
    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    
    # ëª¨ë¸ êµ¬ì¡° ìš”ì•½ ì¶œë ¥
    model.summary()
    
    # 5. í•™ìŠµ ë„ìš°ë¯¸ ì„¤ì • (Callbacks)
    callbacks = [
        # ì‹œí—˜ ì˜ ë³¼ ë•Œë§ˆë‹¤ ì €ì¥í•´! (ê¸°ì¤€: val_lossê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        ModelCheckpoint(MODEL_SAVE_PATH, save_best_only=True, monitor='val_loss', mode='min'),
        # ì„±ì ì´ ë” ì•ˆ ì˜¤ë¥´ë©´ ê·¸ë§Œí•´! (ì‹œê°„ ë‚­ë¹„ ë°©ì§€)
        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    ]
    
    # 6. ì§„ì§œ í•™ìŠµ ì‹œì‘! (Fit)
    print("\nğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤! (ì‹œê°„ì´ ì¢€ ê±¸ë¦½ë‹ˆë‹¤)")
    history = model.fit(
        train_generator,
        epochs=EPOCHS,
        callbacks=callbacks,
        validation_data=validation_generator # ê²€ì¦ìš© ë°ì´í„° ì¶”ê°€
    )
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {MODEL_SAVE_PATH}")

if __name__ == "__main__":
    main()
